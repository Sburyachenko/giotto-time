{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T18:44:30.927583Z",
     "start_time": "2020-04-14T18:44:30.912369Z"
    }
   },
   "source": [
    "## Basic ARMA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic ARMA(p, q) model can be presented as:\n",
    "    \\begin{equation}\n",
    "    x_t = \\mu + \\varepsilon_t +  \\sum_{i=1}^p \\phi_i x_{t-i} + \\sum_{i=1}^q \\theta_i e_{t-i}\n",
    "    \\end{equation}\n",
    "where $\\varepsilon_i$ are error terms which are expected to form a white noise process (i.e. are independent identically distributed random variables with $\\varepsilon_i \\sim N(0, \\sigma^2)$).\n",
    "The key obstacle in analysing and fitting an ARMA model is that error terms $\\varepsilon_i$ are not observable, nor can they be simply calculated as it is in case of, for example, linear regression ($\\varepsilon = Y - \\alpha - \\beta X$). The equation above \n",
    "    \\begin{equation}\n",
    "    \\varepsilon_t = x_t - (\\mu + \\sum_{i=1}^p \\phi_i x_{t-i} + \\sum_{i=1}^q \\theta_i e_{t-i})\n",
    "    \\end{equation}\n",
    "essencially makes $\\varepsilon_t$ a function of $\\{\\varepsilon_{t-1}...\\varepsilon_{t-p}\\}$ which, in turn, can only be calculated recursivelly through previous values of $\\varepsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit the model to available data $X_n = \\{X_1, ..., X_n\\}$ and find optimal parameters $\\mu, \\sigma, \\phi,  \\theta$ we need to maximize the likelihood function, which means to find a set of parameters, for which the joint probability of $\\{X_1, ..., X_n\\}$ is the highest:\n",
    "    \\begin{equation}\n",
    "    L = f(X_n | \\mu, \\sigma, \\phi,  \\theta)\n",
    "    \\end{equation}\n",
    "which is, from what we know of conditional probabilities, equivalent to:\n",
    "    \\begin{equation}\n",
    "    L = \\prod_{i=1}^n f(X_i | X_{i-1}, \\mu, \\sigma, \\phi,  \\theta)\n",
    "    \\end{equation}\n",
    "or\n",
    "    \\begin{equation}\n",
    "    l = \\ln(L) = \\sum_{i=1}^n f(X_i | X_{i-1}, \\mu, \\sigma, \\phi,  \\theta)\n",
    "    \\end{equation}\n",
    "If we assume that the errors $\\varepsilon$ are Gaussian then $x_t$ are also expected to be Gaussian as a linear combination of $\\varepsilon$, $x_0$ and some fixed model parameters. Which means that:\n",
    "    \\begin{equation}\n",
    "    l = \\sum_{i=1}^n -\\frac{1}{2} \\big( \\ln(2\\pi|F_i|) + \\nu_i F_i^{-1}\\nu_i \\big) \n",
    "    \\end{equation}\n",
    "where\n",
    "    \\begin{equation}\n",
    "    \\nu_t = x_t - E(x_t|x_1...x_{t-1}) \\quad and \\quad F_t = E(\\nu_t \\nu_t' |x_1...x_{t-1}).\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State space representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The problem can be reformulated in matrix form using a state space representation with $m$-dimensional vectors $\\alpha_t$ such that:\n",
    "    \\begin{equation}\n",
    "    \\alpha_t^i = \\phi_i x_{t-1} + \\alpha_{t-1}^i + \\theta_{i-1} \\varepsilon_t\n",
    "    \\end{equation}\n",
    "where $m = \\max(p, q+1)$ and where the coefficients $\\phi_i$ for $p<i\\le m$ and $\\theta_i$ for $q<i\\le m$ are assumed to be $0$.\n",
    "\n",
    "It allows us to split our system into **transition equation** which can be interpreted as its true state under assumed ARMA(p, q) mode:\n",
    "    \\begin{equation}\n",
    "    \\alpha_t = K\\alpha_{t-1} + R \\varepsilon_t*\n",
    "    \\end{equation}\n",
    "where:\n",
    "    \\begin{equation}\n",
    "    K=\n",
    "      \\begin{bmatrix}\n",
    "        \\phi_1 & 1 & 0 & ... & 0 \\\\\n",
    "        \\phi_2 & 0 & 1 & ... & 0 \\\\\n",
    "        \\vdots & \\vdots & \\vdots & ...  & \\vdots \\\\ \n",
    "        \\phi_m & 0 & 0 & ... & 0\n",
    "      \\end{bmatrix}\n",
    "       \\quad and \\quad\n",
    "     R=\n",
    "      \\begin{bmatrix}\n",
    "        1 & 0 & 0 & ... & 0 \\\\\n",
    "        0 & \\theta_1 & 0 & ... & 0 \\\\\n",
    "        \\vdots & \\vdots & \\vdots & ...  & \\vdots \\\\ \n",
    "        0 & 0 & 0 & ... & \\theta_m\n",
    "      \\end{bmatrix}\n",
    "    \\end{equation}\n",
    "  and **measurement equation**, where \n",
    "      \\begin{equation}\n",
    "    \\alpha_t^1 = \\phi_1 x_{t-1} + \\alpha_{t-1}^1 + \\varepsilon_t =\n",
    "        \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + \\alpha_{t-2}^1 + \\theta_1 \\varepsilon_{t-1} +  \\varepsilon_t = ... =\n",
    "        x_t - \\mu\n",
    "    \\end{equation}\n",
    "  or $x_t = Z \\alpha_t$ + $\\mu$ where $Z = \\begin{bmatrix}1 & 0 & ... & 0 \\end{bmatrix}$.\n",
    "    \n",
    "    \n",
    "<font color='red'>*Here I interpret $\\varepsilon_t$ as a vector of errors while $R$ is a square matrix. In literature it is also shown as a vector of $\\theta$ multiplied by a constant error but in this case Kalman filter update equation would have a dimensionality mismatch.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kalman filter is a recursive algorithm which allows us to forecast the distribution of $x_t | x_1...x_{t-1}$ so that we can calculate $\\nu_t$ and $F_t$ and the likelihood function. The algorithm works with $\\alpha_t$ and consists of two steps:\n",
    "- predicting the estimate for the next $\\alpha_t$ and its correlation matrix $P_t$ :\n",
    "    \\begin{equation}\n",
    "    \\hat{\\alpha_t} = K\\alpha_{t-1} \\\\\n",
    "    \\hat{P_t} = KP_{t-1}K' + RQR' \\\\\n",
    "    \\hat{x_t} = Z\\hat{a_t} + \\mu\n",
    "    \\end{equation}\n",
    "- updating the values of $\\alpha_t$ and $P_t$ based on actual $x_t$:\n",
    "    \\begin{equation}\n",
    "    \\Gamma_t = \\hat{P_t} Z' F_t^{-1} \\\\\n",
    "    P_{t} = \\hat{P_t} - \\Gamma_t Z  \\hat{P_t}'\\\\\n",
    "    \\alpha_t = \\hat{\\alpha_t} +  \\Gamma_t \\nu_t\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nu$ and $F$ obtained via Kalman filter procedure\n",
    "    \\begin{equation}\n",
    "    \\nu_t = x_t - \\hat{x_t} \\\\\n",
    "    F = Z \\hat{P_t} Z' \\\\\n",
    "    \\end{equation}\n",
    "allow us to calculate $l$ and thus to minimize it using one of the numerical minimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "class KalmanFilter:\n",
    "\n",
    "    def __init__(self, mu, sigma, phi, theta):\n",
    "        self.phi = phi\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        m = len(phi)\n",
    "        R = np.concatenate((np.ones(1), self.theta[:-1]))\n",
    "        self.R = np.diag(R)\n",
    "        K = np.concatenate((self.phi[:-1].reshape((-1, 1)), np.identity(m - 1)), axis=1)\n",
    "        self.K = np.concatenate((K, np.concatenate((self.phi[-1:], np.zeros(m - 1))).reshape(1, -1)))\n",
    "        self.Q = sigma ** 2 * np.identity(m)\n",
    "        self.Z = np.zeros((1, m))\n",
    "        self.Z[0, 0] = 1.0\n",
    "\n",
    "    def update(self, a_hat, p_hat, F, nu):\n",
    "        gain = multi_dot([p_hat, np.transpose(self.Z), np.linalg.inv(F)])\n",
    "        a = a_hat + multi_dot([gain, nu])\n",
    "        p = p_hat - multi_dot([gain, self.Z, np.transpose(p_hat)])\n",
    "        return a, p\n",
    "\n",
    "    def predict(self, a, p, x):\n",
    "        a_hat = np.matmul(self.K, a)\n",
    "        p_hat = mat_square(p, self.K) + mat_square(self.Q, self.R)\n",
    "        x_hat = np.matmul(self.Z, a_hat) + self.mu\n",
    "        F = mat_square(p_hat, self.Z)\n",
    "        nu = x - x_hat\n",
    "        return a_hat, p_hat, x_hat, F, nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _likelihood(X, mu, sigma, phi, theta, errors=False):\n",
    "\n",
    "    loglikelihood = 0.0\n",
    "    m = phi.size\n",
    "    kalman = KalmanFilter(mu, sigma, phi, theta)\n",
    "    p = np.identity(m)\n",
    "    a = np.zeros((m, 1))\n",
    "    eps = np.zeros(len(X))\n",
    "    for i, x in enumerate(X):\n",
    "        a_hat, p_hat, x_hat, F, nu = kalman.predict(a, p, x)\n",
    "        LL_last = -0.5 * (np.log(2 * np.pi * np.abs(F)) + mat_square(np.linalg.inv(F), nu))\n",
    "        a, p = kalman.update(a_hat, p_hat, F, nu)\n",
    "        eps[i] = nu\n",
    "        loglikelihood += LL_last\n",
    "    if errors:\n",
    "        return eps\n",
    "    else:\n",
    "        return -float(loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional sum of squares (CSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional sum of squares is a simplification of state space representation with an assumption that initial unobserved value of $\\alpha_0$ is known (for example $\\alpha_0 = 0$). In this case instead of estimating both expected value and covariance matrix of $\\alpha_t$ we can perform explicit calculations:\n",
    "\\begin{equation}\n",
    "\\varepsilon^*_t = x_t - Z\\hat{\\alpha_t} - \\mu \\quad \\text{where} \\quad \\hat{\\alpha_t} = K \\hat{\\alpha_{t-1}} + K R \\varepsilon^*_{t-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and thus:\n",
    "    \\begin{equation}\n",
    "    l = \\sum_{i=1}^n -\\frac{1}{2} \\big( \\ln(2\\pi\\sigma^2) + \\frac{\\varepsilon_i^* \\varepsilon_i^*}{\\sigma^2} \\big)\n",
    "      =  -\\frac{1}{2} \\big( n \\ln(2\\pi\\sigma^2) + \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\varepsilon_i^{*2} \\big)\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>I still don't fully understand how these manipulations turned $F$ (which is $\\hat{P}[1, 1]$) into a simple $\\sigma$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is a much simple model yet it is proven to asymptotically converge to MLE estimates[6].\n",
    "Recursive error calculation can be represented as an IIR filter[7] where $x_t$ is an input signal and $\\varepsilon^*_t$ is output signal. It allows us to use ``lfilter()`` from ``scipy.signal`` package which performs such calculation effeciently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_css(params, X, len_p, errors=False, transform=True):\n",
    "\n",
    "    if len(params.shape) > 1:\n",
    "        print(params.shape)\n",
    "\n",
    "    mu = params[0]\n",
    "    nobs = len(X) - len_p\n",
    "    phi = np.r_[1, params[2:len_p + 2]]\n",
    "    theta = np.r_[1, params[len_p + 2:]]\n",
    "\n",
    "    y = X - mu\n",
    "    eps = lfilter(phi, theta, y)\n",
    "    if errors:\n",
    "        return eps\n",
    "    else:\n",
    "        ssr = np.dot(eps, eps)\n",
    "        sigma2 = ssr / nobs\n",
    "        loglikelihood = -nobs / 2. * (np.log(2 * np.pi * sigma2)) - ssr / (2. * sigma2)\n",
    "        return -loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity and invertability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arima model is stationary if absolute value of all the roots of the polynom:\n",
    "\\begin{equation}\n",
    "    \\phi(z) = 1 - \\sum_{i=1}^{p}\\phi_i z^i\n",
    "\\end{equation}\n",
    "is greater than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arima model is invertible (i.e. can be converted to an AR($\\infty$) model)  if absolute value of all the roots of the polynom:\n",
    "\\begin{equation}\n",
    "    \\theta(z) = 1 + \\sum_{i=1}^{p}\\theta_i z^i\n",
    "\\end{equation}\n",
    "is greater than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materials:\n",
    " 1. https://otexts.com/fpp2/non-seasonal-arima.html - basic problem formulation\n",
    " 2. https://uh.edu/~bsorense/kalman.pdf - \n",
    " 3. https://www.stat.purdue.edu/~chong/stat520/ps/statespace.pdf\n",
    " 4. http://www.stat.ucla.edu/~frederic/221/W17/221ch3.pdf\n",
    " 5. https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python - general information about filters\n",
    " 6. https://www.nuffield.ox.ac.uk/economics/Papers/1997/w6/ma.pdf - relationship between CSS and MLE\n",
    " 7. https://en.wikipedia.org/wiki/Infinite_impulse_response - general information on IIR\n",
    " 8. http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/08_intro_arma.pdf - general ARIMA information + stationarity conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giotto",
   "language": "python",
   "name": "giotto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
